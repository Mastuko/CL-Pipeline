{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb642c53-5e1c-4625-99ea-4f23ec8005da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torchvision.datasets.utils as dataset_utils\n",
    "from site import addsitedir\n",
    "addsitedir(\"/home/wangqihang/MyContinualLearning/visual\")\n",
    "from utils import metrics, datasets, nn_utils\n",
    "import utils.others as others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49d21cd",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c1d172f-8021-4ad3-98d3-ba3fdf83c033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "def split_feature_label_datasets(files, p=0.5):\n",
    "    for feature_file, label_file in files:\n",
    "        # Extract the base filename without extension for saving the split datasets\n",
    "        feature_base_name = os.path.basename(feature_file)\n",
    "        label_base_name = os.path.basename(label_file)\n",
    "        feature_name_without_extension = os.path.splitext(feature_base_name)[0]\n",
    "        label_name_without_extension = os.path.splitext(label_base_name)[0]\n",
    "        \n",
    "        # Read the CSV files\n",
    "        feature_data = pd.read_csv(feature_file, header=None)\n",
    "        label_data = pd.read_csv(label_file, header=None)\n",
    "        \n",
    "        # Perform the split\n",
    "        train_feature_data, test_feature_data, train_label_data, test_label_data = train_test_split(\n",
    "            feature_data, label_data, test_size=p, random_state=42)\n",
    "        \n",
    "        # Save the mini train and test datasets\n",
    "        train_feature_file = os.path.join(os.path.dirname(feature_file), f\"mini_train_{feature_name_without_extension}.csv\")\n",
    "        test_feature_file = os.path.join(os.path.dirname(feature_file), f\"mini_test_{feature_name_without_extension}.csv\")\n",
    "        train_label_file = os.path.join(os.path.dirname(label_file), f\"mini_train_{label_name_without_extension}.csv\")\n",
    "        test_label_file = os.path.join(os.path.dirname(label_file), f\"mini_test_{label_name_without_extension}.csv\")\n",
    "        \n",
    "        train_feature_data.to_csv(train_feature_file, index=False, header=False)\n",
    "        test_feature_data.to_csv(test_feature_file, index=False, header=False)\n",
    "        train_label_data.to_csv(train_label_file, index=False, header=False)\n",
    "        test_label_data.to_csv(test_label_file, index=False, header=False)\n",
    "\n",
    "PREPATH = \"/home/wangqihang/MyContinualLearning/store/datasets/QoT/\"\n",
    "\n",
    "TRAINs = []\n",
    "\n",
    "CHANNEL_TRAINs = []\n",
    "CHANNEL_TESTs = []\n",
    "\n",
    "TIMEVAR_TRAINs = []\n",
    "TIMEVAR_TESTs = []\n",
    "\n",
    "DROPED_TRAINs = []\n",
    "DROPED_TESTs = []\n",
    "for i in range(5):\n",
    "    TRAINs.append((PREPATH + f\"X_train_{str(i+1)}.csv\", PREPATH + f\"y_train_{str(i+1)}.csv\"))\n",
    "\n",
    "for i in range(5):\n",
    "    CHANNEL_TESTs.append((PREPATH + f\"X_test_1_{str(i+1)}.csv\", PREPATH + f\"y_test_1_{str(i+1)}.csv\"))\n",
    "split_feature_label_datasets(CHANNEL_TESTs, p=0.5)\n",
    "CHANNEL_TESTs = []\n",
    "for i in range(5):\n",
    "    CHANNEL_TESTs.append((PREPATH + f\"mini_test_X_test_1_{str(i+1)}.csv\", PREPATH + f\"mini_test_y_test_1_{str(i+1)}.csv\"))\n",
    "    CHANNEL_TRAINs.append((PREPATH + f\"mini_train_X_test_1_{str(i+1)}.csv\", PREPATH + f\"mini_train_y_test_1_{str(i+1)}.csv\"))\n",
    "\n",
    "for i in range(6):\n",
    "    TIMEVAR_TESTs.append((PREPATH + f\"X_test_2_{str(i+1)}.csv\", PREPATH + f\"y_test_2_{str(i+1)}.csv\"))\n",
    "split_feature_label_datasets(TIMEVAR_TESTs, p=0.5)\n",
    "TIMEVAR_TESTs = []\n",
    "for i in range(6):\n",
    "    TIMEVAR_TESTs.append((PREPATH + f\"mini_test_X_test_2_{str(i+1)}.csv\", PREPATH + f\"mini_test_y_test_2_{str(i+1)}.csv\"))\n",
    "    TIMEVAR_TRAINs.append((PREPATH + f\"mini_train_X_test_2_{str(i+1)}.csv\", PREPATH + f\"mini_train_y_test_2_{str(i+1)}.csv\"))\n",
    "\n",
    "for i in range(3):\n",
    "    DROPED_TESTs.append((PREPATH + f\"X_test_3_{str(i+1)}.csv\", PREPATH + f\"y_test_3_{str(i+1)}.csv\"))\n",
    "split_feature_label_datasets(DROPED_TESTs, p=0.5)\n",
    "DROPED_TESTs = []\n",
    "for i in range(3):\n",
    "    DROPED_TESTs.append((PREPATH + f\"mini_test_X_test_3_{str(i+1)}.csv\", PREPATH + f\"mini_test_y_test_3_{str(i+1)}.csv\"))\n",
    "    DROPED_TRAINs.append((PREPATH + f\"mini_train_X_test_3_{str(i+1)}.csv\", PREPATH + f\"mini_train_y_test_3_{str(i+1)}.csv\"))\n",
    "\n",
    "split_feature_label_datasets([[PREPATH + \"X_train_1.csv\", PREPATH + \"y_train_1.csv\"]], p=0.2)\n",
    "\n",
    "Train_dataset = nn_utils.dataset(PREPATH + \"mini_train_X_train_1.csv\", PREPATH + \"mini_train_y_train_1.csv\", preprocessing=True)\n",
    "Train_loader = nn_utils.loader(Train_dataset, batch_size=len(Train_dataset), shuffle=False)\n",
    "\n",
    "Test_dataset = nn_utils.dataset(PREPATH + \"mini_test_X_train_1.csv\", PREPATH + \"mini_test_y_train_1.csv\", transform=[Train_dataset.X_scaler, Train_dataset.y_scaler])\n",
    "Test_loader = nn_utils.loader(Test_dataset, batch_size=len(Test_dataset), shuffle=False)\n",
    "\n",
    "Channel_train_loader = []\n",
    "Time_train_loader = []\n",
    "Drop_train_loader = []\n",
    "\n",
    "Channel_test_loader = []\n",
    "Time_test_loader = []\n",
    "Drop_test_loader = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_dataset = nn_utils.dataset(*CHANNEL_TESTs[i], transform=[Train_dataset.X_scaler, Train_dataset.y_scaler])\n",
    "    test_dataloader = nn_utils.loader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "    Channel_test_loader.append(test_dataloader)\n",
    "    train_dataset = nn_utils.dataset(*CHANNEL_TRAINs[i], transform=[Train_dataset.X_scaler, Train_dataset.y_scaler])\n",
    "    train_dataloader = nn_utils.loader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
    "    Channel_train_loader.append(train_dataloader)\n",
    "\n",
    "for i in range(6):\n",
    "    test_dataset = nn_utils.dataset(*TIMEVAR_TESTs[i], transform=[Train_dataset.X_scaler, Train_dataset.y_scaler])\n",
    "    test_dataloader = nn_utils.loader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "    Time_test_loader.append(test_dataloader)\n",
    "    train_dataset = nn_utils.dataset(*TIMEVAR_TRAINs[i], transform=[Train_dataset.X_scaler, Train_dataset.y_scaler])\n",
    "    train_dataloader = nn_utils.loader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
    "    Time_train_loader.append(train_dataloader)\n",
    "    \n",
    "for i in range(3):\n",
    "    test_dataset = nn_utils.dataset(*DROPED_TESTs[i], transform=[Train_dataset.X_scaler, Train_dataset.y_scaler])\n",
    "    test_dataloader = nn_utils.loader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "    Drop_test_loader.append(test_dataloader)\n",
    "    train_dataset = nn_utils.dataset(*DROPED_TRAINs[i], transform=[Train_dataset.X_scaler, Train_dataset.y_scaler])\n",
    "    train_dataloader = nn_utils.loader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
    "    Drop_train_loader.append(train_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789ed9de",
   "metadata": {},
   "source": [
    "## Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "573f4745-af01-4938-ab7f-98a6758bbfeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def get_std(a, b):\n",
    "    a = a.numpy()\n",
    "    b = b.numpy()\n",
    "    return np.sqrt(np.var(a - b))\n",
    "\n",
    "def get_99std(a, b):\n",
    "    a = a.numpy()\n",
    "    b = b.numpy()\n",
    "    return np.percentile(np.abs(a-b), 99)\n",
    "\n",
    "def get_100std(a, b):\n",
    "    a = a.numpy()\n",
    "    b = b.numpy()\n",
    "    return np.percentile(np.abs(a-b), 100)\n",
    "\n",
    "def save_list_to_csv(data_list, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        for row in data_list:\n",
    "            csv_writer.writerow([row])\n",
    "\n",
    "def save_dict_to_csv(data_dict, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        for key, value in data_dict.items():\n",
    "            csv_writer.writerow([key, value])\n",
    "\n",
    "def test_model(model, device, test_loader):\n",
    "    model.eval()\n",
    "    loss = nn.MSELoss()\n",
    "    var = metrics.StdDeviation()\n",
    "    test_loss = 0\n",
    "    # test_var = 0\n",
    "    mse = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            cnt += 1\n",
    "            data, target = data.to(device), target.to(device).float()\n",
    "            output = model(data)\n",
    "            test_loss += loss(output, target).item()\n",
    "            test_var = var(output.cpu(), target.cpu())# sum up batch loss\n",
    "\n",
    "    test_loss /= cnt\n",
    "\n",
    "    return test_loss, test_var\n",
    "\n",
    "def erm_train(model, device, train_loader, optimizer, epoch):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    mseloss = nn.MSELoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = 2 * mseloss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "993c18b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "from random import sample\n",
    "from threading import Thread\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "\n",
    "def get_scenario_agnostic_dataloaders(task_index, given_sequence):\n",
    "    '''\n",
    "    Returns the current and replay dataloaders irrespective of the scenario, \n",
    "    allowing for a sequence that switches between different scenarios.\n",
    "\n",
    "    :param task_index: The index of the current task in the sequence\n",
    "    :param given_sequence: A list of tuples, each containing ('Scenario', task_index)\n",
    "    '''\n",
    "    # Mapping scenario names to dataloaders\n",
    "    scenario_to_dataloader_train = {\n",
    "        'Channel': Channel_train_loader,\n",
    "        'Time': Time_train_loader,\n",
    "        'Drop': Drop_train_loader\n",
    "    }\n",
    "    scenario_to_dataloader_test = {\n",
    "        'Channel': Channel_test_loader,\n",
    "        'Time': Time_test_loader,\n",
    "        'Drop': Drop_test_loader\n",
    "    }\n",
    "    \n",
    "    current_scenario, current_task = given_sequence[task_index]\n",
    "    current_train_dataloader = scenario_to_dataloader_train[current_scenario][current_task]\n",
    "    current_test_dataloader = scenario_to_dataloader_train[current_scenario][current_task]\n",
    "    \n",
    "    # Collecting past tasks from the given sequence up to the current task index for replay\n",
    "    past_tasks = given_sequence[:task_index]\n",
    "    replay_train_dataloaders = [scenario_to_dataloader_train[scenario][task] for scenario, task in past_tasks]\n",
    "    replay_test_dataloaders = [scenario_to_dataloader_test[scenario][task] for scenario, task in past_tasks]\n",
    "    \n",
    "    return current_train_dataloader, current_test_dataloader, replay_train_dataloaders, replay_test_dataloaders\n",
    "\n",
    "def generate_random_sequence(num_tasks, scenarios=['Channel', 'Time', 'Drop']):\n",
    "    '''\n",
    "    Generates a random sequence of tasks and scenarios.\n",
    "\n",
    "    :param num_tasks: The total number of tasks in the sequence\n",
    "    :param scenarios: The list of possible scenarios\n",
    "    '''\n",
    "    if num_tasks == 500:\n",
    "        _Channel_train_loader = Channel_train_loader[:2]\n",
    "        _Time_train_loader = Time_train_loader[:2]\n",
    "    elif num_tasks == 1000:\n",
    "        _Channel_train_loader = Channel_train_loader[:4]\n",
    "        _Time_train_loader = Time_train_loader[:4]\n",
    "    else:\n",
    "        _Channel_train_loader = Channel_train_loader\n",
    "        _Time_train_loader = Time_train_loader\n",
    "    scenario_to_dataloader = {\n",
    "        'Channel': _Channel_train_loader,\n",
    "        'Time': _Time_train_loader,\n",
    "        'Drop': Drop_train_loader\n",
    "    }\n",
    "    sequence = []\n",
    "    for _ in range(num_tasks):\n",
    "        scenario = random.choice(scenarios)\n",
    "        task_index = random.randint(0, len(scenario_to_dataloader[scenario]) - 1)\n",
    "        sequence.append((scenario, task_index))\n",
    "    return sequence\n",
    "\n",
    "def save_model(model, path='/home/wangqihang/MyContinualLearning/store/models/base_model.pth'):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(model, path='/home/wangqihang/MyContinualLearning/store/models/base_model.pth'):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n",
    "def optimizer_to(optim, device):\n",
    "    for param in optim.state.values():\n",
    "        # Not sure there are any global tensors in the state dict\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            param.data = param.data.to(device)\n",
    "            if param._grad is not None:\n",
    "                param._grad.data = param._grad.data.to(device)\n",
    "        elif isinstance(param, dict):\n",
    "            for subparam in param.values():\n",
    "                if isinstance(subparam, torch.Tensor):\n",
    "                    subparam.data = subparam.data.to(device)\n",
    "                    if subparam._grad is not None:\n",
    "                        subparam._grad.data = subparam._grad.data.to(device)\n",
    "\n",
    "# Placeholder function for OOD detection based on test loss\n",
    "def ood_detection(model, loss_hist, test_loader, loss_function, threshold=0.5):\n",
    "    '''\n",
    "    Detects if the model is Out-Of-Distribution (OOD) based on the test loss.\n",
    "    :param model: Trained ML model\n",
    "    :param test_loader: DataLoader for the test set\n",
    "    :param loss_function: Loss function used in training\n",
    "    :param threshold: Loss threshold for OOD detection\n",
    "    :return: Boolean indicating if retraining is needed\n",
    "    '''\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    loss_hist.append(avg_loss)\n",
    "    return avg_loss > threshold\n",
    "\n",
    "def train_without_replay_without_retrain(model, device, loss_function, optimizer, sequence, retrain_threshold=0.1):\n",
    "    '''\n",
    "    Trains the model on a sequence of tasks, with retraining but without replay.\n",
    "    :param model: The ML model to be trained\n",
    "    :param loss_function: Loss function used in training\n",
    "    :param optimizer: Optimizer used in training\n",
    "    :param sequence: Sequence of tasks (scenario, task index) to train on\n",
    "    :param retrain_threshold: Loss threshold for triggering retraining\n",
    "    '''\n",
    "    retrain_count = 0\n",
    "    loss_hist = []\n",
    "    for task_index in range(len(sequence)):\n",
    "        # Double each task in the sequence for OOD detection and then retraining/test\n",
    "        current_train_loader, current_test_loader, replay_train_loaders, replay_test_loaders = get_scenario_agnostic_dataloaders(task_index, sequence)\n",
    "        \n",
    "        # OOD Detection\n",
    "        if ood_detection(model, loss_hist, current_test_loader, loss_function, retrain_threshold):\n",
    "            retrain_count += 1\n",
    "            # Retraining\n",
    "            \n",
    "    return loss_hist, retrain_count\n",
    "\n",
    "def train_without_replay_with_retrain(model, device, loss_function, optimizer, sequence, retrain_threshold=0.1):\n",
    "    '''\n",
    "    Trains the model on a sequence of tasks, with retraining but without replay.\n",
    "    :param model: The ML model to be trained\n",
    "    :param loss_function: Loss function used in training\n",
    "    :param optimizer: Optimizer used in training\n",
    "    :param sequence: Sequence of tasks (scenario, task index) to train on\n",
    "    :param retrain_threshold: Loss threshold for triggering retraining\n",
    "    '''\n",
    "    \n",
    "    retrain_count = 0\n",
    "    loss_hist = []\n",
    "    times = []\n",
    "    for task_index in range(len(sequence)):\n",
    "        # Double each task in the sequence for OOD detection and then retraining/test\n",
    "        current_train_loader, current_test_loader, replay_train_loaders, replay_test_loaders = get_scenario_agnostic_dataloaders(task_index, sequence)\n",
    "        start_time = time.time()\n",
    "        # OOD Detection\n",
    "        if ood_detection(model, loss_hist, current_test_loader, loss_function, retrain_threshold):\n",
    "            retrain_count += 1\n",
    "            # Retraining\n",
    "            model.to(device)\n",
    "            model.train()\n",
    "            for data, target in current_train_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = loss_function(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        times.append(execution_time)\n",
    "    return loss_hist, retrain_count, times\n",
    "\n",
    "\n",
    "def train_with_replay_with_retrain(model, device, loss_function, optimizer, sequence, retrain_threshold=0.1):\n",
    "    '''\n",
    "    Trains the model on a sequence of tasks, with both retraining and replay.\n",
    "    :param model: The ML model to be trained\n",
    "    :param loss_function: Loss function used in training\n",
    "    :param optimizer: Optimizer used in training\n",
    "    :param sequence: Sequence of tasks (scenario, task index) to train on\n",
    "    :param retrain_threshold: Loss threshold for triggering retraining\n",
    "    '''\n",
    "    retrain_count = 0\n",
    "    replay_memory = []  # To store past training data for replay\n",
    "    loss_hist = []\n",
    "    times = []\n",
    "    mem = []\n",
    "    \n",
    "    for task_index in range(len(sequence)):\n",
    "        # Double each task in the sequence for OOD detection and then retraining/test\n",
    "        current_train_loader, current_test_loader, replay_train_loaders, replay_test_loaders = get_scenario_agnostic_dataloaders(task_index, sequence)\n",
    "        # print(f\"task {task_index} on going\")\n",
    "        # OOD Detection\n",
    "        start_time = time.time()\n",
    "        if ood_detection(model, loss_hist, current_test_loader, loss_function, retrain_threshold):\n",
    "            retrain_count += 1\n",
    "            \n",
    "            # Create a concatenated dataset of current task and replay memory\n",
    "            concat_dataset = ConcatDataset([current_train_loader.dataset] + replay_memory)\n",
    "            concat_loader = DataLoader(concat_dataset, batch_size=len(concat_dataset), shuffle=True)  # Adjust batch size as needed\n",
    "            # Retraining\n",
    "            model.to(device)\n",
    "            model.train()\n",
    "            for data, target in concat_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = loss_function(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Update Replay Memory\n",
    "            sample_data = torch.utils.data.Subset(current_train_loader.dataset, [i for i in range(len(current_train_loader.dataset) // 3)])\n",
    "            replay_memory.append(sample_data)\n",
    "\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        times.append(execution_time)\n",
    "\n",
    "        tempfile = open('tempfile', 'wb')\n",
    "        pickle.dump(replay_memory, tempfile)\n",
    "        tempfile.close()\n",
    "        mem.append(os.path.getsize('tempfile')/1000000)\n",
    "    \n",
    "    return loss_hist, retrain_count, times, mem\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, intermidiate_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Build encoder model\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        self.dense1 = nn.Linear(44, 128)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.dense2 = nn.Linear(128, intermidiate_dim)\n",
    "        self.act2 = nn.Tanh()\n",
    "\n",
    "        self.model = nn.Sequential(self.dense1, self.act1,\n",
    "                                   self.dense2, self.act2)\n",
    "        \n",
    "        # Posterior on Y; probabilistic regressor\n",
    "        self.dense_mu_y = nn.Linear(intermidiate_dim, 1)\n",
    "        self.dense_logvar_y = nn.Linear(intermidiate_dim, 1)\n",
    "\n",
    "        # q(z|x)\n",
    "        self.dense_mu_z = nn.Linear(intermidiate_dim, latent_dim)\n",
    "        self.dense_logvar_z = nn.Linear(intermidiate_dim, latent_dim)\n",
    "\n",
    "        # latent generator\n",
    "        self.dense_gen_z = nn.Linear(1, latent_dim)\n",
    "\n",
    "    def sampling(self, mu, log_var):\n",
    "        # Reparameterize\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        # z\n",
    "        mu_z = self.dense_mu_z(out)\n",
    "        logvar_z = self.dense_logvar_z(out)\n",
    "        z = self.sampling(mu_z, logvar_z)\n",
    "        # y\n",
    "        mu_y = self.dense_mu_y(out)\n",
    "        logvar_y = self.dense_logvar_y(out)\n",
    "        y = self.sampling(mu_y, logvar_y)\n",
    "\n",
    "        # y conditional z\n",
    "        z_bar_y = self.dense_gen_z(y)\n",
    "\n",
    "        return [mu_z, logvar_z, z], [mu_y, logvar_y, y], z_bar_y\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, intermidiate_dim, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.dense1 = nn.Linear(latent_dim, intermidiate_dim)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.dense2 = nn.Linear(intermidiate_dim, 128)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.dense3 = nn.Linear(128, 44)\n",
    "\n",
    "        self.model = nn.Sequential(self.dense1, self.act1,\n",
    "                                   self.dense2, self.act2,\n",
    "                                   self.dense3)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class EnhancedGenerativeModel:\n",
    "    def __init__(self, intermidiate_dim=32, latent_dim=8, lr=0.0001, batch_size=32):\n",
    "        self.encoder = Encoder(intermidiate_dim, latent_dim).cuda(device='cuda:1')\n",
    "        self.decoder = Decoder(intermidiate_dim, latent_dim).cuda(device='cuda:1')\n",
    "        self.optimizer = optim.Adam(list(self.encoder.parameters()) + list(self.decoder.parameters()), lr=lr)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def update(self, current_train_loader):\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "        \n",
    "        # Generate synthetic data\n",
    "        num_samples = len(current_train_loader.dataset)\n",
    "        synthetic_data = self.generate_data(num_samples)\n",
    "        synthetic_labels = torch.zeros((num_samples, 1))\n",
    "\n",
    "        # Create a DataLoader for the synthetic data\n",
    "        synthetic_dataset = TensorDataset(synthetic_data, synthetic_labels)\n",
    "        synthetic_loader = DataLoader(synthetic_dataset, batch_size=len(synthetic_dataset), shuffle=True)\n",
    "\n",
    "        # Concatenate the feature and label in the row direction from current_train_loader\n",
    "        concatenated_data_list = []\n",
    "        for data, target in current_train_loader:\n",
    "            concatenated_data = torch.cat((data, target), dim=1)  # Assuming target is column vector\n",
    "            if len(data) != 0 and len(target) != 0:\n",
    "                concatenated_data_list.append(concatenated_data)\n",
    "        concatenated_data_tensor = torch.vstack(concatenated_data_list)\n",
    "        \n",
    "        # Create labels for the concatenated data (these can be zero or whatever is appropriate)\n",
    "        concatenated_labels = torch.zeros(concatenated_data_tensor.shape[0], 1)\n",
    "\n",
    "        # Create a DataLoader for the concatenated data\n",
    "        concatenated_dataset = TensorDataset(concatenated_data_tensor, concatenated_labels)\n",
    "        concatenated_loader = DataLoader(concatenated_dataset, batch_size=len(concatenated_dataset), shuffle=True)\n",
    "\n",
    "        # Combine synthetic and real (concatenated) data\n",
    "        combined_dataset = ConcatDataset([concatenated_dataset, synthetic_dataset])\n",
    "        combined_loader = DataLoader(combined_dataset, batch_size=len(combined_dataset), shuffle=True)\n",
    "\n",
    "        for ep in range(10):\n",
    "            for batch_data, batch_target in combined_loader:\n",
    "                batch_data, batch_target = batch_data.cuda(device='cuda:1'), batch_target.cuda(device='cuda:1')\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass through encoder\n",
    "                [mu_z, logvar_z, z], [mu_y, logvar_y, y], z_bar_y = self.encoder(batch_data)\n",
    "\n",
    "                # Decode the latent variable\n",
    "                x_hat = self.decoder(z)\n",
    "\n",
    "                # Loss computation (Reconstruction + KL divergence)\n",
    "                reconstruction_loss = F.mse_loss(x_hat, batch_data, reduction='sum')\n",
    "                kl_loss = -0.5 * torch.sum(1 + logvar_z - mu_z.pow(2) - logvar_z.exp())\n",
    "                loss = reconstruction_loss + kl_loss\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    def generate_data(self, num_samples):\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "\n",
    "        # Generate latent variables (you might need to adjust dimensions)\n",
    "        z = torch.randn(num_samples, 8).cuda(device='cuda:1')\n",
    "\n",
    "        # Generate data using the decoder\n",
    "        with torch.no_grad():\n",
    "            generated_data = self.decoder(z)\n",
    "        \n",
    "        # Move generated data to CPU\n",
    "        generated_data = generated_data.cpu()\n",
    "        \n",
    "        return generated_data\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save({\n",
    "            'encoder': self.encoder.state_dict(),\n",
    "            'decoder': self.decoder.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict()\n",
    "        }, path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.encoder.load_state_dict(checkpoint['encoder'])\n",
    "        self.decoder.load_state_dict(checkpoint['decoder'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "def unpack_generated_data(generated_data):\n",
    "    '''\n",
    "    Unpacks the generated synthetic data into features and labels.\n",
    "    \n",
    "    :param generated_data: Tensor containing the generated synthetic data where features and labels are concatenated.\n",
    "    \n",
    "    :returns: A tuple (features, labels)\n",
    "    '''\n",
    "    # Assuming the generated data is of shape [num_samples, feature_dim + label_dim]\n",
    "    # and that the label is a single column at the end of the data tensor\n",
    "    features = generated_data[:, :-1]  # All columns except the last one\n",
    "    labels = generated_data[:, -1].unsqueeze(-1)  # The last column\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "# Placeholder function for OOD detection based on test loss\n",
    "def ood_detection(model, loss_hist, test_loader, loss_function, threshold=0.2):\n",
    "    '''\n",
    "    Detects if the model is Out-Of-Distribution (OOD) based on the test loss.\n",
    "    :param model: Trained ML model\n",
    "    :param test_loader: DataLoader for the test set\n",
    "    :param loss_function: Loss function used in training\n",
    "    :param threshold: Loss threshold for OOD detection\n",
    "    :return: Boolean indicating if retraining is needed\n",
    "    '''\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    loss_hist.append(avg_loss)\n",
    "    return avg_loss > threshold\n",
    "\n",
    "\n",
    "def train_with_gen_with_retrain_multithreaded(model, device, generative_model, loss_function, optimizer, sequence, retrain_threshold=0.1):\n",
    "    '''\n",
    "    Trains the model on a sequence of tasks, with both retraining and data generated from a generative model.\n",
    "    Uses multi-threading to train the generative model and main model in parallel.\n",
    "    :param model: The ML model to be trained\n",
    "    :param generative_model: The generative model for simulating past data\n",
    "    :param loss_function: Loss function used in training\n",
    "    :param optimizer: Optimizer used in training\n",
    "    :param sequence: Sequence of tasks (scenario, task index) to train on\n",
    "    :param retrain_threshold: Loss threshold for triggering retraining\n",
    "    '''\n",
    "    retrain_count = 0\n",
    "    loss_hist = []\n",
    "    times = []\n",
    "    mem = []\n",
    "    \n",
    "    def train_generative_model(train_loader):\n",
    "        generative_model.update(train_loader)\n",
    "    \n",
    "    gen_thread = None  # Initialize generative model thread as None\n",
    "\n",
    "    for task_index in range(len(sequence)):\n",
    "        current_train_loader, current_test_loader, _, _ = get_scenario_agnostic_dataloaders(task_index, sequence)\n",
    "        start_time = time.time()\n",
    "        if ood_detection(model, loss_hist, current_test_loader, loss_function, retrain_threshold):\n",
    "            retrain_count += 1\n",
    "\n",
    "            # If a previous generative model thread is running, wait for it to finish\n",
    "            if gen_thread is not None and gen_thread.is_alive():\n",
    "                gen_thread.join()\n",
    "\n",
    "            # Generate data using the generative model\n",
    "            gen_data, gen_target = unpack_generated_data(generative_model.generate_data(len(current_train_loader.dataset)))\n",
    "            gen_dataset = TensorDataset(gen_data, gen_target)\n",
    "            # gen_loader = DataLoader(gen_dataset, batch_size=32, shuffle=True)\n",
    "            \n",
    "            # Create a concatenated dataset of current task and generated data\n",
    "            concat_dataset = ConcatDataset([current_train_loader.dataset, gen_dataset])\n",
    "            # concat_dataset = ConcatDataset([current_train_loader.dataset])\n",
    "            concat_loader = DataLoader(concat_dataset, batch_size=len(concat_dataset), shuffle=True)\n",
    "            \n",
    "            # Retraining the main model\n",
    "            model.to(device)\n",
    "            model.train()\n",
    "            \n",
    "            for data, target in concat_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = loss_function(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Start a new thread for training the generative model\n",
    "            gen_thread = Thread(target=train_generative_model, args=(current_train_loader,))\n",
    "            gen_thread.start()\n",
    "\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        times.append(execution_time)\n",
    "        \n",
    "        tempfile = open('tempfile', 'wb')\n",
    "        pickle.dump(generative_model, tempfile)\n",
    "        tempfile.close()\n",
    "        mem.append(os.path.getsize('tempfile')/1000000)\n",
    "\n",
    "    return loss_hist, retrain_count, times, mem\n",
    "\n",
    "def calculate_eta(T_m, S_m, epsilon_m, alpha=1.0, beta=1.0, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Calculate the efficiency metric eta for different retraining strategies in optical networks.\n",
    "    \n",
    "    Parameters:\n",
    "    - T0 (float): Initial training time\n",
    "    - S0 (float): Initial required storage space\n",
    "    - T_m (list of float): List of retraining times for each iteration\n",
    "    - S_m (list of float): List of required storage spaces for each iteration\n",
    "    - epsilon_m (list of float): List of errors for each iteration\n",
    "    - N (int): Total number of retrainings counted once after the entire sequence has been traversed\n",
    "    - alpha (float, optional): Control parameter for the trade-off between model performance and resource usage\n",
    "    - beta (float, optional): Control parameter for the impact of the total number of retrainings\n",
    "\n",
    "    Returns:\n",
    "    - eta (float): The efficiency metric\n",
    "    \"\"\"\n",
    "    \n",
    "    M = len(T_m)  # Total number of retraining iterations\n",
    "    \n",
    "    numerator = np.sum(np.exp(- (alpha * np.array(epsilon_m) + beta * np.array(T_m) + gamma * np.array(S_m))))\n",
    "    denominator = M\n",
    "    \n",
    "    eta = numerator / denominator\n",
    "    \n",
    "    return eta\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db6d40e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "layer=8\n",
    "hidden_num=[32, 64, 64, 64, 64, 32, 16]\n",
    "# hidden_num = [32, 64, 16, 16, 16, 16, 32]\n",
    "dropout=[0.01, 0.1, 0.2, 0.1, 0.1, 0.01, 0.01]\n",
    "model = nn_utils.NeuralNetwork(43, 1, layer, hidden_num, dropout).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0)\n",
    "\n",
    "model = load_model(model)\n",
    "\n",
    "# Example usage\n",
    "generative_model = EnhancedGenerativeModel()\n",
    "generative_model.update(Train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5b4b1b2-8f5b-40dd-b621-72a908b58a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_once(model, sequence, alpha=1, beta=0.1, gamma=0.1):\n",
    "    seq_len = len(sequence)\n",
    "    loss_function = nn.MSELoss()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    # SI\n",
    "    model = load_model(model)\n",
    "    SI_optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0)\n",
    "    SI_loss, _ = train_without_replay_without_retrain(model, device, loss_function, SI_optimizer, sequence)\n",
    "    SI_eta = calculate_eta([0 for i in range(seq_len)], [0 for i in range(seq_len)], [0 if i<0.2 else i for i in SI_loss], alpha, beta, gamma)\n",
    "    SI_MSE = np.mean(SI_loss)\n",
    "\n",
    "    # AR\n",
    "    model = load_model(model)\n",
    "    AR_optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0)\n",
    "    AR_loss, AR_retrain, AR_time = train_without_replay_with_retrain(model, device, loss_function, AR_optimizer, sequence)\n",
    "    AR_eta = calculate_eta(AR_time, [0 for i in range(seq_len)], [0 if i<0.2 else i for i in AR_loss], alpha, beta, gamma)\n",
    "    AR_MSE = np.mean(AR_loss)\n",
    "\n",
    "    # ARER\n",
    "    model = load_model(model)\n",
    "    ARER_optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0)\n",
    "    ARER_loss, ARER_retrain, ARER_time, ARER_mem = train_with_replay_with_retrain(model, device, loss_function, ARER_optimizer, sequence)\n",
    "    ARER_eta = calculate_eta(ARER_time, ARER_mem, [0 if i<0.2 else i for i in ARER_loss], alpha, beta, gamma)\n",
    "    ARER_MSE = np.mean(ARER_loss)\n",
    "\n",
    "    # ARER_DGM\n",
    "    model = load_model(model)\n",
    "    ARER_DGM_optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0)\n",
    "    ARER_DGM_loss, ARER_DGM_retrain, ARER_DGM_time, ARER_DGM_mem = train_with_gen_with_retrain_multithreaded(model, device, generative_model, loss_function, ARER_DGM_optimizer, sequence)\n",
    "    ARER_DGM_eta = calculate_eta(ARER_DGM_time, ARER_DGM_mem, [0 if i<0.2 else i for i in ARER_DGM_loss], alpha, beta, gamma)\n",
    "    ARER_DGM_MSE = np.mean(ARER_DGM_loss)\n",
    "\n",
    "    Results = {}\n",
    "    Results['Len'] = seq_len\n",
    "    Results['SI'] = {}\n",
    "    Results['SI']['eta'] = SI_eta\n",
    "    Results['SI']['loss'] = SI_MSE\n",
    "    Results['SI']['retrain'] = 0\n",
    "\n",
    "    Results['AR'] = {}\n",
    "    Results['AR']['eta'] = AR_eta\n",
    "    Results['AR']['loss'] = AR_MSE\n",
    "    Results['AR']['retrain'] = AR_retrain\n",
    "\n",
    "    Results['ARER'] = {}\n",
    "    Results['ARER']['eta'] = ARER_eta\n",
    "    Results['ARER']['loss'] = ARER_MSE\n",
    "    Results['ARER']['retrain'] = ARER_retrain\n",
    "\n",
    "    Results['ARER_DGM'] = {}\n",
    "    Results['ARER_DGM']['eta'] = ARER_DGM_eta\n",
    "    Results['ARER_DGM']['loss'] = ARER_DGM_MSE\n",
    "    Results['ARER_DGM']['retrain'] = ARER_DGM_retrain\n",
    "\n",
    "    return Results\n",
    "\n",
    "random_sequence_1 = generate_random_sequence(1000, scenarios=['Channel', 'Time'])\n",
    "results_1 = train_once(model, random_sequence_1, alpha=10, beta=0.01, gamma=0.001)\n",
    "random_sequence_2 = generate_random_sequence(500, scenarios=['Channel', 'Time'])\n",
    "results_2 = train_once(model, random_sequence_2, alpha=10, beta=0.01, gamma=0.001)\n",
    "random_sequence_3 = generate_random_sequence(2000, scenarios=['Channel', 'Time'])\n",
    "results_3 = train_once(model, random_sequence_3, alpha=10, beta=0.01, gamma=0.001)\n",
    "\n",
    "save_dict_to_csv(results_1, '/home/wangqihang/MyContinualLearning/store/results/results_s1_1000.csv')\n",
    "save_dict_to_csv(results_2, '/home/wangqihang/MyContinualLearning/store/results/results_s1_500.csv')\n",
    "save_dict_to_csv(results_3, '/home/wangqihang/MyContinualLearning/store/results/results_s1_2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73bbc2ea-07b3-4d8b-980e-55da521d79bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sequence_2 = generate_random_sequence(2000, scenarios=['Channel', 'Time'])\n",
    "seq_len = len(random_sequence_2)\n",
    "loss_function = nn.MSELoss()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "alpha=10\n",
    "beta=1\n",
    "gamma=0.01\n",
    "\n",
    "# SI\n",
    "model = load_model(model)\n",
    "SI_optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0)\n",
    "SI_loss, _ = train_without_replay_without_retrain(model, device, loss_function, SI_optimizer, random_sequence_2)\n",
    "SI_eta = calculate_eta([0 for i in range(seq_len)], [0 for i in range(seq_len)], SI_loss, alpha, beta, gamma)\n",
    "SI_MSE = np.mean(SI_loss)\n",
    "\n",
    "# AR\n",
    "model = load_model(model)\n",
    "AR_optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0)\n",
    "AR_loss, AR_retrain, AR_time = train_without_replay_with_retrain(model, device, loss_function, AR_optimizer, random_sequence_2)\n",
    "AR_eta = calculate_eta(AR_time, [0 for i in range(seq_len)], AR_loss, alpha, beta, gamma)\n",
    "AR_MSE = np.mean(AR_loss)\n",
    "\n",
    "# ARER\n",
    "model = load_model(model)\n",
    "ARER_optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0)\n",
    "ARER_loss, ARER_retrain, ARER_time, ARER_mem = train_with_replay_with_retrain(model, device, loss_function, ARER_optimizer, random_sequence_2)\n",
    "ARER_eta = calculate_eta(ARER_time, ARER_mem, ARER_loss, alpha, beta, gamma)\n",
    "ARER_MSE = np.mean(ARER_loss)\n",
    "\n",
    "# ARER_DGM\n",
    "model = load_model(model)\n",
    "ARER_DGM_optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0)\n",
    "ARER_DGM_loss, ARER_DGM_retrain, ARER_DGM_time, ARER_DGM_mem = train_with_gen_with_retrain_multithreaded(model, device, generative_model, loss_function, ARER_DGM_optimizer, random_sequence_2)\n",
    "ARER_DGM_eta = calculate_eta(ARER_DGM_time, ARER_DGM_mem, ARER_DGM_loss, alpha, beta, gamma)\n",
    "ARER_DGM_MSE = np.mean(ARER_DGM_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40a477cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=10\n",
    "beta=0.1\n",
    "gamma=0.1\n",
    "SI_eta = calculate_eta([0 for i in range(seq_len)], [0 for i in range(seq_len)], [0 if i<0.2 else i for i in SI_loss], alpha, beta, gamma)\n",
    "SI_MSE = np.mean(SI_loss)\n",
    "AR_eta = calculate_eta(AR_time, [0 for i in range(seq_len)], [0 if i<0.2 else i for i in AR_loss], alpha, beta, gamma)\n",
    "AR_MSE = np.mean(AR_loss)\n",
    "ARER_eta = calculate_eta(ARER_time, ARER_mem, [0 if i<0.2 else i for i in ARER_loss], alpha, beta, gamma)\n",
    "ARER_MSE = np.mean(ARER_loss)\n",
    "ARER_DGM_eta = calculate_eta(ARER_DGM_time, ARER_DGM_mem, [0 if i<0.2 else i for i in ARER_DGM_loss], alpha, beta, gamma)\n",
    "ARER_DGM_MSE = np.mean(ARER_DGM_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a71bb4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = {}\n",
    "Results['Len'] = seq_len\n",
    "Results['SI'] = {}\n",
    "Results['SI']['eta'] = SI_eta\n",
    "Results['SI']['loss'] = SI_MSE\n",
    "Results['SI']['retrain'] = 0\n",
    "\n",
    "Results['AR'] = {}\n",
    "Results['AR']['eta'] = AR_eta\n",
    "Results['AR']['loss'] = AR_MSE\n",
    "Results['AR']['retrain'] = AR_retrain\n",
    "\n",
    "Results['ARER'] = {}\n",
    "Results['ARER']['eta'] = ARER_eta\n",
    "Results['ARER']['loss'] = ARER_MSE\n",
    "Results['ARER']['retrain'] = ARER_retrain\n",
    "\n",
    "Results['ARER_DGM'] = {}\n",
    "Results['ARER_DGM']['eta'] = ARER_DGM_eta\n",
    "Results['ARER_DGM']['loss'] = ARER_DGM_MSE\n",
    "Results['ARER_DGM']['retrain'] = ARER_DGM_retrain\n",
    "\n",
    "save_dict_to_csv(Results, '/home/wangqihang/MyContinualLearning/store/results/results_101001_1000.csv')\n",
    "save_dict_to_csv(Results, '/home/wangqihang/MyContinualLearning/store/results/results_101001_500.csv')\n",
    "save_dict_to_csv(results_3, '/home/wangqihang/MyContinualLearning/store/results/results_101001_2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2759773f-f19e-436f-bf90-b19154f23b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7345302021404506"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARER_eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b91d9c7-0b07-4f76-b1ff-745463cd22a7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6167477456804493"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SI_eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be3cfb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7547625041316729"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AR_eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645dd87b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e54f470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
